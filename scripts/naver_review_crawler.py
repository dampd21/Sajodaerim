#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë¦¬ë·° í¬ë¡¤ëŸ¬
- ë°©ë¬¸ì ë¦¬ë·° + ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜ì§‘
- ì§€ì ë³„ ìˆ˜ì§‘
- ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (ì¦ë¶„ ìˆ˜ì§‘)
"""

import os
import sys
import json
import time
import re
import hashlib
from datetime import datetime

print("=" * 60, flush=True)
print("ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë¦¬ë·° í¬ë¡¤ëŸ¬", flush=True)
print("=" * 60, flush=True)

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    print("[INFO] Selenium ë¡œë“œ ì™„ë£Œ", flush=True)
except ImportError as e:
    print(f"[ERROR] Selenium í•„ìš”: {e}", flush=True)
    sys.exit(1)

try:
    from webdriver_manager.chrome import ChromeDriverManager
    print("[INFO] WebDriver Manager ë¡œë“œ ì™„ë£Œ", flush=True)
except ImportError as e:
    print(f"[ERROR] WebDriver Manager í•„ìš”: {e}", flush=True)
    sys.exit(1)

# ============================================
# ì§€ì ë³„ Place ID
# ============================================
STORE_PLACES = {
    "ì—­ëŒ€ì§¬ë½• ë³¸ì ": "1542530224",
    "ì—­ëŒ€ì§¬ë½• ë³‘ì ì ": "1870047654",
    "ì—­ëŒ€ì§¬ë½• ì†¡íŒŒì ": "2066998075",
    "ì—­ëŒ€ì§¬ë½• ë‹¤ì‚°1í˜¸ì ": "1455516190",
    "ì—­ëŒ€ì§¬ë½• í™”ì„±ë°˜ì›”ì ": "1474983307",
    "ì—­ëŒ€ì§¬ë½• ì˜¤ì‚°ì‹œì²­ì ": "1160136895",
    "ì—­ëŒ€ì§¬ë½• ë‘ì •ì ": "1726445983",
    "ì—­ëŒ€ì§¬ë½• ì†¡íƒ„ì ": "1147851109",
    "ì—­ëŒ€ì§¬ë½• ì—¬ìˆ˜êµ­ë™ì ": "1773140342",
}


def setup_driver():
    """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
    print("[SETUP] Chrome ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...", flush=True)
    
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument(
        '--user-agent=Mozilla/5.0 (Linux; Android 10; SM-G975F) '
        'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36'
    )
    
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("[SETUP] Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ", flush=True)
        return driver
    except Exception as e:
        print(f"[ERROR] Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}", flush=True)
        raise


def scroll_to_load(driver, max_scrolls=10, wait_time=2):
    """ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ë¦¬ë·° ë¡œë“œ"""
    last_height = driver.execute_script("return document.body.scrollHeight")
    scroll_count = 0
    
    while scroll_count < max_scrolls:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(wait_time)
        
        new_height = driver.execute_script("return document.body.scrollHeight")
        
        if new_height == last_height:
            break
        
        last_height = new_height
        scroll_count += 1
    
    return scroll_count


def parse_date(date_str):
    """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
    if not date_str:
        return None
    
    parts = re.findall(r'\d+', date_str)
    
    if len(parts) >= 3:
        year = int(parts[0])
        if year < 100:
            year += 2000
        month = int(parts[1])
        day = int(parts[2])
        return f"{year}-{month:02d}-{day:02d}"
    elif len(parts) >= 2:
        current_year = datetime.now().year
        month = int(parts[0])
        day = int(parts[1])
        return f"{current_year}-{month:02d}-{day:02d}"
    
    return date_str


def generate_review_id(review):
    """ë¦¬ë·° ê³ ìœ  ID ìƒì„±"""
    author = review.get('author', '')[:20]
    content = review.get('content', '')[:50]
    date = review.get('visit_date', '') or review.get('write_date', '')
    
    raw = f"{author}_{content}_{date}"
    return hashlib.md5(raw.encode()).hexdigest()[:16]


# ============================================
# ë°©ë¬¸ì ë¦¬ë·° íŒŒì‹±
# ============================================

def parse_visitor_reviews(driver):
    """ë°©ë¬¸ì ë¦¬ë·° íŒŒì‹±"""
    reviews = []
    
    try:
        review_items = driver.find_elements(By.CSS_SELECTOR, 'li.place_apply_pui')
        print(f"[PARSE] ë°©ë¬¸ì ë¦¬ë·° {len(review_items)}ê°œ ë°œê²¬", flush=True)
        
        for item in review_items:
            try:
                review = {'type': 'visitor'}
                
                # ì‘ì„±ìëª…
                try:
                    review['author'] = item.find_element(By.CSS_SELECTOR, '.pui__NMi-Dp').text.strip()
                except:
                    review['author'] = ''
                
                # ë¦¬ë·° ë‚´ìš©
                try:
                    review['content'] = item.find_element(By.CSS_SELECTOR, '.pui__vn15t2 a').text.strip()
                except:
                    review['content'] = ''
                
                # ë°©ë¬¸ í‚¤ì›Œë“œ
                try:
                    keyword_els = item.find_elements(By.CSS_SELECTOR, '.pui__V8F9nN em')
                    review['keywords'] = [el.text.strip() for el in keyword_els if el.text.strip()]
                except:
                    review['keywords'] = []
                
                # íƒœê·¸
                try:
                    tag_els = item.find_elements(By.CSS_SELECTOR, '.pui__jhpEyP')
                    tags = []
                    for tag_el in tag_els:
                        text = tag_el.text.strip()
                        if text and not text.startswith('+'):
                            tags.append(text)
                    review['tags'] = tags
                except:
                    review['tags'] = []
                
                # ë°©ë¬¸ì¼
                try:
                    date_el = item.find_element(By.CSS_SELECTOR, '.pui__gfuUIT time')
                    raw_date = date_el.text.strip()
                    review['visit_date_raw'] = raw_date
                    review['visit_date'] = parse_date(raw_date)
                except:
                    review['visit_date_raw'] = ''
                    review['visit_date'] = ''
                
                # ë°©ë¬¸ ì •ë³´
                try:
                    info_els = item.find_elements(By.CSS_SELECTOR, '.pui__gfuUIT')
                    review['visit_info'] = [el.text.strip() for el in info_els if el.text.strip()]
                except:
                    review['visit_info'] = []
                
                # ì´ë¯¸ì§€
                try:
                    img_els = item.find_elements(By.CSS_SELECTOR, '.K0PDV')
                    images = []
                    for img_el in img_els:
                        src = img_el.get_attribute('src')
                        if src and 'pstatic.net' in src:
                            images.append(src)
                    review['images'] = images[:5]
                except:
                    review['images'] = []
                
                review['id'] = generate_review_id(review)
                
                if review['author'] or review['content']:
                    reviews.append(review)
                    
            except Exception as e:
                continue
        
    except Exception as e:
        print(f"[ERROR] ë°©ë¬¸ì ë¦¬ë·° íŒŒì‹± ì‹¤íŒ¨: {e}", flush=True)
    
    return reviews


# ============================================
# ë¸”ë¡œê·¸ ë¦¬ë·° íŒŒì‹±
# ============================================

def parse_blog_reviews(driver):
    """ë¸”ë¡œê·¸ ë¦¬ë·° íŒŒì‹±"""
    reviews = []
    
    try:
        review_items = driver.find_elements(By.CSS_SELECTOR, 'li.EblIP')
        print(f"[PARSE] ë¸”ë¡œê·¸ ë¦¬ë·° {len(review_items)}ê°œ ë°œê²¬", flush=True)
        
        for item in review_items:
            try:
                review = {'type': 'blog'}
                
                # ë¸”ë¡œê·¸ ë§í¬
                try:
                    link_el = item.find_element(By.CSS_SELECTOR, 'a.behIY')
                    review['blog_url'] = link_el.get_attribute('href') or ''
                except:
                    review['blog_url'] = ''
                
                # ì‘ì„±ìëª…
                try:
                    review['author'] = item.find_element(By.CSS_SELECTOR, '.pui__NMi-Dp').text.strip()
                except:
                    review['author'] = ''
                
                # ë¸”ë¡œê·¸ëª…
                try:
                    review['blog_name'] = item.find_element(By.CSS_SELECTOR, '.XR_ao').text.strip()
                except:
                    review['blog_name'] = ''
                
                # ë¸”ë¡œê·¸ ê¸€ ì œëª©
                try:
                    review['title'] = item.find_element(By.CSS_SELECTOR, '.pui__dGLDWy').text.strip()
                except:
                    review['title'] = ''
                
                # ë¦¬ë·° ë‚´ìš©
                try:
                    review['content'] = item.find_element(By.CSS_SELECTOR, '.pui__vn15t2 span').text.strip()
                except:
                    review['content'] = ''
                
                # ì‘ì„±ì¼
                try:
                    date_el = item.find_element(By.CSS_SELECTOR, '.u5XwJ time')
                    raw_date = date_el.text.strip()
                    review['write_date_raw'] = raw_date
                    review['write_date'] = parse_date(raw_date)
                except:
                    review['write_date_raw'] = ''
                    review['write_date'] = ''
                
                # ì´ë¯¸ì§€
                try:
                    img_els = item.find_elements(By.CSS_SELECTOR, '.K0PDV')
                    images = []
                    for img_el in img_els:
                        src = img_el.get_attribute('src')
                        if src and 'pstatic.net' in src:
                            images.append(src)
                    review['images'] = images[:5]
                except:
                    review['images'] = []
                
                review['id'] = generate_review_id(review)
                
                if review['author'] or review['content'] or review['title']:
                    reviews.append(review)
                    
            except Exception as e:
                continue
        
    except Exception as e:
        print(f"[ERROR] ë¸”ë¡œê·¸ ë¦¬ë·° íŒŒì‹± ì‹¤íŒ¨: {e}", flush=True)
    
    return reviews


# ============================================
# ì§€ì ë³„ ë¦¬ë·° ìˆ˜ì§‘
# ============================================

def crawl_store_reviews(driver, store_name, place_id, max_reviews=50):
    """íŠ¹ì • ì§€ì ì˜ ë°©ë¬¸ì + ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜ì§‘"""
    print(f"\n{'='*50}", flush=True)
    print(f"[CRAWL] {store_name} (ID: {place_id})", flush=True)
    print(f"{'='*50}", flush=True)
    
    store_data = {
        'store_name': store_name,
        'place_id': place_id,
        'visitor_reviews': [],
        'blog_reviews': [],
        'visitor_count': 0,
        'blog_count': 0,
        'crawled_at': datetime.now().isoformat()
    }
    
    # 1. ë°©ë¬¸ì ë¦¬ë·° ìˆ˜ì§‘
    visitor_url = f"https://m.place.naver.com/restaurant/{place_id}/review/visitor?reviewSort=recent"
    print(f"[CRAWL] ë°©ë¬¸ì ë¦¬ë·° URL: {visitor_url}", flush=True)
    
    try:
        driver.get(visitor_url)
        time.sleep(3)
        
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'li.place_apply_pui'))
            )
            print("[CRAWL] ë°©ë¬¸ì ë¦¬ë·° í˜ì´ì§€ ë¡œë”© ì™„ë£Œ", flush=True)
        except TimeoutException:
            print("[WARN] ë°©ë¬¸ì ë¦¬ë·° ë¡œë”© íƒ€ì„ì•„ì›ƒ - ë¦¬ë·°ê°€ ì—†ì„ ìˆ˜ ìˆìŒ", flush=True)
        
        scroll_count = scroll_to_load(driver, max_scrolls=max(1, max_reviews // 10))
        print(f"[CRAWL] ìŠ¤í¬ë¡¤ {scroll_count}íšŒ ì™„ë£Œ", flush=True)
        
        visitor_reviews = parse_visitor_reviews(driver)
        store_data['visitor_reviews'] = visitor_reviews[:max_reviews]
        store_data['visitor_count'] = len(store_data['visitor_reviews'])
        print(f"[CRAWL] ë°©ë¬¸ì ë¦¬ë·° {store_data['visitor_count']}ê°œ ìˆ˜ì§‘", flush=True)
        
    except Exception as e:
        print(f"[ERROR] ë°©ë¬¸ì ë¦¬ë·° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}", flush=True)
    
    time.sleep(2)
    
    # 2. ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜ì§‘
    blog_url = f"https://m.place.naver.com/restaurant/{place_id}/review/ugc?type=photoView&reviewSort=recent"
    print(f"[CRAWL] ë¸”ë¡œê·¸ ë¦¬ë·° URL: {blog_url}", flush=True)
    
    try:
        driver.get(blog_url)
        time.sleep(3)
        
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'li.EblIP'))
            )
            print("[CRAWL] ë¸”ë¡œê·¸ ë¦¬ë·° í˜ì´ì§€ ë¡œë”© ì™„ë£Œ", flush=True)
        except TimeoutException:
            print("[WARN] ë¸”ë¡œê·¸ ë¦¬ë·° ë¡œë”© íƒ€ì„ì•„ì›ƒ - ë¦¬ë·°ê°€ ì—†ì„ ìˆ˜ ìˆìŒ", flush=True)
        
        scroll_count = scroll_to_load(driver, max_scrolls=max(1, max_reviews // 10))
        print(f"[CRAWL] ìŠ¤í¬ë¡¤ {scroll_count}íšŒ ì™„ë£Œ", flush=True)
        
        blog_reviews = parse_blog_reviews(driver)
        store_data['blog_reviews'] = blog_reviews[:max_reviews]
        store_data['blog_count'] = len(store_data['blog_reviews'])
        print(f"[CRAWL] ë¸”ë¡œê·¸ ë¦¬ë·° {store_data['blog_count']}ê°œ ìˆ˜ì§‘", flush=True)
        
    except Exception as e:
        print(f"[ERROR] ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}", flush=True)
    
    print(f"[RESULT] {store_name}: ë°©ë¬¸ì {store_data['visitor_count']}ê°œ + ë¸”ë¡œê·¸ {store_data['blog_count']}ê°œ", flush=True)
    
    return store_data


# ============================================
# ë°ì´í„° ë³‘í•©
# ============================================

def merge_reviews(existing_reviews, new_reviews):
    """ê¸°ì¡´ ë¦¬ë·°ì™€ ìƒˆ ë¦¬ë·° ë³‘í•© (ì¤‘ë³µ ì œê±°)"""
    existing_ids = {r.get('id') for r in existing_reviews if r.get('id')}
    
    merged = list(existing_reviews)
    added = 0
    
    for review in new_reviews:
        review_id = review.get('id')
        if review_id and review_id not in existing_ids:
            merged.append(review)
            existing_ids.add(review_id)
            added += 1
    
    # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    def get_date(r):
        return r.get('visit_date') or r.get('write_date') or ''
    
    merged.sort(key=get_date, reverse=True)
    
    return merged, added


def load_existing_data(file_path):
    """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[WARN] ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}", flush=True)
    return None


def save_data(data, file_path):
    """ë°ì´í„° ì €ì¥"""
    dir_path = os.path.dirname(file_path)
    if dir_path:
        os.makedirs(dir_path, exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"[SAVE] {file_path}", flush=True)


# ============================================
# ë©”ì¸
# ============================================

def main():
    print(f"\nì‹œì‘: {datetime.now()}", flush=True)
    print(f"ìˆ˜ì§‘ ëŒ€ìƒ: {len(STORE_PLACES)}ê°œ ì§€ì ", flush=True)
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    existing_data = load_existing_data('docs/review_data.json')
    if existing_data:
        print(f"[INFO] ê¸°ì¡´ ë°ì´í„° ë°œê²¬ - ë³‘í•© ëª¨ë“œ", flush=True)
    else:
        print(f"[INFO] ê¸°ì¡´ ë°ì´í„° ì—†ìŒ - ì‹ ê·œ ìˆ˜ì§‘", flush=True)
    
    result = {
        'generated_at': datetime.now().isoformat(),
        'platform': 'naver',
        'stores': [],
        'summary': {
            'total_stores': 0,
            'total_visitor_reviews': 0,
            'total_blog_reviews': 0,
            'total_reviews': 0,
            'new_visitor_reviews': 0,
            'new_blog_reviews': 0
        }
    }
    
    driver = None
    
    try:
        driver = setup_driver()
        
        for store_name, place_id in STORE_PLACES.items():
            store_data = crawl_store_reviews(driver, store_name, place_id, max_reviews=100)
            
            # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
            if existing_data:
                existing_store = next(
                    (s for s in existing_data.get('stores', []) if s['store_name'] == store_name),
                    None
                )
                
                if existing_store:
                    # ë°©ë¬¸ì ë¦¬ë·° ë³‘í•©
                    merged_visitor, added_visitor = merge_reviews(
                        existing_store.get('visitor_reviews', []),
                        store_data['visitor_reviews']
                    )
                    store_data['visitor_reviews'] = merged_visitor
                    store_data['visitor_count'] = len(merged_visitor)
                    result['summary']['new_visitor_reviews'] += added_visitor
                    print(f"[MERGE] {store_name} ë°©ë¬¸ì: ê¸°ì¡´ {len(existing_store.get('visitor_reviews', []))} + ì‹ ê·œ {added_visitor} = ì´ {store_data['visitor_count']}", flush=True)
                    
                    # ë¸”ë¡œê·¸ ë¦¬ë·° ë³‘í•©
                    merged_blog, added_blog = merge_reviews(
                        existing_store.get('blog_reviews', []),
                        store_data['blog_reviews']
                    )
                    store_data['blog_reviews'] = merged_blog
                    store_data['blog_count'] = len(merged_blog)
                    result['summary']['new_blog_reviews'] += added_blog
                    print(f"[MERGE] {store_name} ë¸”ë¡œê·¸: ê¸°ì¡´ {len(existing_store.get('blog_reviews', []))} + ì‹ ê·œ {added_blog} = ì´ {store_data['blog_count']}", flush=True)
            
            result['stores'].append(store_data)
            result['summary']['total_visitor_reviews'] += store_data['visitor_count']
            result['summary']['total_blog_reviews'] += store_data['blog_count']
            
            # ì§€ì  ê°„ ëŒ€ê¸°
            time.sleep(3)
        
        result['summary']['total_stores'] = len(result['stores'])
        result['summary']['total_reviews'] = (
            result['summary']['total_visitor_reviews'] + 
            result['summary']['total_blog_reviews']
        )
        
        # ì €ì¥
        os.makedirs('docs', exist_ok=True)
        os.makedirs('output', exist_ok=True)
        
        save_data(result, 'docs/review_data.json')
        save_data(result, 'output/review_data.json')
        
        print("\n" + "=" * 60, flush=True)
        print("ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!", flush=True)
        print("=" * 60, flush=True)
        print(f"  ğŸ“ ì§€ì : {result['summary']['total_stores']}ê°œ", flush=True)
        print(f"  ğŸ‘¤ ë°©ë¬¸ì ë¦¬ë·°: {result['summary']['total_visitor_reviews']}ê°œ", flush=True)
        print(f"  ğŸ“ ë¸”ë¡œê·¸ ë¦¬ë·°: {result['summary']['total_blog_reviews']}ê°œ", flush=True)
        print(f"  ğŸ“Š ì´ ë¦¬ë·°: {result['summary']['total_reviews']}ê°œ", flush=True)
        if existing_data:
            print(f"  ğŸ†• ì‹ ê·œ ë°©ë¬¸ì: +{result['summary']['new_visitor_reviews']}ê°œ", flush=True)
            print(f"  ğŸ†• ì‹ ê·œ ë¸”ë¡œê·¸: +{result['summary']['new_blog_reviews']}ê°œ", flush=True)
        print("=" * 60, flush=True)
        
    except Exception as e:
        print(f"\n[ERROR] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}", flush=True)
        import traceback
        traceback.print_exc()
        sys.exit(1)
        
    finally:
        if driver:
            driver.quit()
            print("[CLEANUP] ë¸Œë¼ìš°ì € ì¢…ë£Œ", flush=True)


if __name__ == "__main__":
    main()
